\subsection*{2.5 Hyperparameter selection}
The hyperparameters that we played with are batch size, state vector size, and the number of epochs spent to train the model. It was noticed that the batch size of 1 gives the best results regardless of the chosen model. The state vector size was \(x^{2}\) where \(x \in [1,2,...,9]\). We noticed that the model stops improving when \(x>7\), and it becomes computationally infeasible given the time frame we were given. It was mentioned in the class that one of the methods of regularization is "early training stopping". For this reason, we ran the program for 20 epochs for all values of x. It was also interesting to notice that batch size of 1 made the best results to occur in early epochs. We claim that this is due to the faster convergence. Finally, we picked the best accuracy on validation set, and re-run the training with the best \(x\) and upto the chosen epoch. 

\subsection*{2.5 RNN implementations}
Different versions of RNN models were implemented, trained and tested. Namely, the following three RNN methods were tried out:
\begin{itemize}
	\item Vanilla RNN
	\item Long-Short Term Memory (LSTM)
	\item Gated Recurrent Unites (GRU)
\end{itemize}

The following three sections briefly report the hyperparameters used for each of the models. The results for all of the models are in the Results section, and summarized in Table \ref{result-table}
	
\subsection*{2.5.1 Vanilla RNN}
The following TensorFlow class was used for Vanilla RNN implementation:\\ \\
\texttt{tf.nn.rnn\_cell.BasicRNNCell}\\ \\
Table \ref{vanilla-table} summarizes the hyperparameter selection for Vanilla RNN training.
\begin{table}[h]
\centering
\begin{tabular}{|l|l|}
\hline
 Batch size & 1 \\ \hline
 State vector size & 128 \\ \hline
 Epoch & 4 \\ \hline
\end{tabular}
\caption{Hyperparameters for Vanilla RNN}
\label{vanilla-table}
\end{table}

\subsection*{2.5.2 LSTM}
The following TensorFlow class was used for LSTM implementation:\\ \\
\texttt{tf.nn.rnn\_cell.LSTMCell}\\ \\
Table \ref{lstm-table} summarizes the hyperparameter selection for RNN training with LSTM.
\begin{table}[h]
\centering
\begin{tabular}{|l|l|}
\hline
 Batch size & 1 \\ \hline
 State vector size & 64 \\ \hline
 Epoch & 2 \\ \hline
\end{tabular}
\caption{Hyperparameters for LSTM}
\label{lstm-table}
\end{table}

\subsection*{2.5.3 GRU}
The following TensorFlow class was used for GRU implementation:\\ \\
\texttt{tf.nn.rnn\_cell.GRUCell}\\ \\
Table \ref{gru-table} summarizes the hyperparameter selection for RNN training with GRU.

\begin{table}[h]
\centering
\begin{tabular}{|l|l|}
\hline
 Batch size & 1 \\ \hline
 State vector size & 256 \\ \hline
 Epoch & 5 \\ \hline
\end{tabular}
\caption{Hyperparameters for GRU}
\label{gru-table}
\end{table}


